[pytest]
addopts =
    --verbose
    --cov=src
    --cov-report=html
    --cov-report=term-missing
    --benchmark-only
    --benchmark-autosave
    --benchmark-compare
    --benchmark-group-by=func
    --benchmark-warmup=on

testpaths = src/tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    benchmark: marks benchmark tests
    integration: marks integration tests
    security: marks security tests
    monitoring: marks monitoring tests
    ml: marks machine learning tests

[coverage:run]
branch = True
source = src

[coverage:report]
exclude_lines =
    pragma: no cover
    def __repr__
    if self.debug:
    raise NotImplementedError
    if __name__ == .__main__.:
    pass
    raise ImportError
    except ImportError:
    raise AssertionError

[coverage:html]
directory = coverage_html

[benchmark]
min_rounds = 100
max_time = 2.0
warmup = True
calibration_precision = 50
disable_gc = True
